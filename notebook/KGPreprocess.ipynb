{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "from rdflib import Graph, URIRef, Literal, XSD\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "from openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d42c0e",
   "metadata": {},
   "source": [
    "# Natural language translation of uniquely identified entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Suppress RDFLib literal datatype conversion warnings ===\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.getLogger(\"rdflib.term\").setLevel(logging.ERROR)\n",
    "\n",
    "def get_last_part(uri):\n",
    "    \"\"\"Extract the last part of a URI (after last / or #)\"\"\"\n",
    "    if isinstance(uri, str):\n",
    "        return re.split(r'[/#]', uri)[-1]\n",
    "    elif isinstance(uri, URIRef):\n",
    "        return re.split(r'[/#]', str(uri))[-1]\n",
    "    return str(uri)\n",
    "\n",
    "def uri_to_qname(graph, uri):\n",
    "    \"\"\"Convert URI to QName (e.g., mwo:description), with fallback to last part\"\"\"\n",
    "    try:\n",
    "        qname = graph.qname(uri)\n",
    "        # Split the qname at ':' and take the last part if it exists\n",
    "        return qname.split(':')[-1] if ':' in qname else qname\n",
    "    except:\n",
    "        return get_last_part(uri)\n",
    "\n",
    "def get_literal_value(literal):\n",
    "    \"\"\"Safely extract the value from a Literal, handling invalid datatypes\"\"\"\n",
    "    try:\n",
    "        return literal.toPython()\n",
    "    except (ValueError, TypeError):\n",
    "        return str(literal).split('^^')[0].strip('\"')\n",
    "\n",
    "def parse_ttl_to_json(ttl_path, output_json=\"../data/MSE-KG/KGPreprocess/MSE_KG.json\", error_log=\"../data/MSE-KG/errors.json\"):\n",
    "    g = Graph()\n",
    "    g.parse(ttl_path, format=\"turtle\")\n",
    "\n",
    "    results = {}\n",
    "    errors = []\n",
    "\n",
    "    for entity in g.subjects():\n",
    "        # Apply the same URI simplification to entity URIs\n",
    "        entity_key = uri_to_qname(g, entity)\n",
    "        results[entity_key] = {}\n",
    "\n",
    "        for predicate, obj in g.predicate_objects(subject=entity):\n",
    "            pred_key = uri_to_qname(g, predicate)\n",
    "\n",
    "            try:\n",
    "                # URI object\n",
    "                if isinstance(obj, URIRef):\n",
    "                    obj_value = uri_to_qname(g, obj)\n",
    "\n",
    "                # Literal object\n",
    "                elif isinstance(obj, Literal):\n",
    "                    if obj.datatype == XSD.time:\n",
    "                        try:\n",
    "                            obj_value = obj.toPython()\n",
    "                        except Exception:\n",
    "                            obj_value = str(obj)\n",
    "                            errors.append({\n",
    "                                \"entity\": entity_key,\n",
    "                                \"predicate\": pred_key,\n",
    "                                \"value\": str(obj),\n",
    "                                \"datatype\": str(obj.datatype),\n",
    "                                \"reason\": \"Invalid xsd:time\"\n",
    "                            })\n",
    "                    else:\n",
    "                        obj_value = get_literal_value(obj)\n",
    "                else:\n",
    "                    obj_value = str(obj)\n",
    "\n",
    "                if not isinstance(obj_value, str):\n",
    "                    obj_value = str(obj_value)\n",
    "\n",
    "                if pred_key not in results[entity_key]:\n",
    "                    results[entity_key][pred_key] = []\n",
    "                results[entity_key][pred_key].append(obj_value)\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append({\n",
    "                    \"entity\": entity_key,\n",
    "                    \"predicate\": pred_key,\n",
    "                    \"value\": str(obj),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    if errors:\n",
    "        with open(error_log, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(errors, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"⚠️  {len(errors)} literal values failed to parse. See {error_log} for details.\")\n",
    "\n",
    "    print(f\"✅ Parsed {len(results)} chunks. Output saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ttl_file = \"../data/MSE-KG/KG_QA/Output.ttl\"  # Modify this path to your TTL file\n",
    "    parse_ttl_to_json(ttl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12763c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kg = pd.read_json(\"../data/MSE-KG/KGPreprocess/MSE_KG.json\", encoding=\"utf-8\").T\n",
    "df_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af8bcd",
   "metadata": {},
   "source": [
    "## Handling Wikidata Identifiers (Q-items) \n",
    "\n",
    "Cite from https://www.wikidata.org/wiki/Wikidata:Identifiers\n",
    "\n",
    "Wikidata identifiers: Each Wikidata entity is identified by an entity ID, which is a number prefixed by a letter. \n",
    "\n",
    "Here, only items, also known as Q-items, are prefixed with Q (e.g. Q12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_label(q_number):\n",
    "    \"\"\"Fetch English label for a Wikidata Q-number\"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{q_number}.json\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'][q_number]\n",
    "        \n",
    "        # Get English label if available\n",
    "        if 'labels' in entity and 'en' in entity['labels']:\n",
    "            return entity['labels']['en']['value']\n",
    "        \n",
    "        # Fallback to first available label\n",
    "        if 'labels' in entity and len(entity['labels']) > 0:\n",
    "            first_label = next(iter(entity['labels'].values()))\n",
    "            return first_label['value']\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch label for {q_number}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def create_label_mapping(kg_data):\n",
    "    \"\"\"Create mapping from Q-numbers to display labels\"\"\"\n",
    "    label_map = {}\n",
    "    for properties in kg_data.values():\n",
    "        # Process Wikidata references in 'sameAs'\n",
    "        if 'sameAs' in properties:\n",
    "            for uri in properties['sameAs']:\n",
    "                if isinstance(uri, str):\n",
    "                    # Handle both Q123 and full URI formats\n",
    "                    if uri.startswith('Q') and uri[1:].isdigit():\n",
    "                        q_number = uri\n",
    "                    elif 'wikidata.org/entity/Q' in uri:\n",
    "                        q_number = uri.split('/')[-1]\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                    if q_number not in label_map:\n",
    "                        sleep(1)  # Respectful API delay\n",
    "                        label = get_wikidata_label(q_number)\n",
    "                        if label:\n",
    "                            label_map[q_number] = label\n",
    "    return label_map\n",
    "\n",
    "def replace_references(data, label_map):\n",
    "    \"\"\"Replace all references with their display labels\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: replace_references(v, label_map) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_references(item, label_map) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Handle both Q123 and full URI formats\n",
    "        if data in label_map:\n",
    "            return label_map[data]\n",
    "        elif data.startswith('Q') and data[1:].isdigit() and data in label_map:\n",
    "            return label_map[data]\n",
    "        elif 'wikidata.org/entity/Q' in data:\n",
    "            q_number = data.split('/')[-1]\n",
    "            return label_map.get(q_number, data)\n",
    "    return data\n",
    "\n",
    "def process_kg_data(input_file, output_file):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        kg_data = json.load(f)\n",
    "    \n",
    "    # Step 1: Create label mapping\n",
    "    label_map = create_label_mapping(kg_data)\n",
    "    \n",
    "    # Step 2: Replace all references\n",
    "    transformed_data = replace_references(kg_data, label_map)\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(transformed_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"../data/MSE-KG/KGPreprocess/MSE_KG.json\"\n",
    "    output_json = \"../data/MSE-KG/KGPreprocess/MSE_KG_Wikidata.json\"\n",
    "    process_kg_data(input_json, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wk = pd.read_json(\"../data/MSE-KG//KGPreprocess/MSE_KG_Wikidata.json\", encoding=\"utf-8\").T\n",
    "df_wk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916b830",
   "metadata": {},
   "source": [
    "## Processes tail entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_label_mapping(kg_data):\n",
    "    \"\"\"Create mapping from entity IDs to display labels based on specified rules\"\"\"\n",
    "    label_map = {}\n",
    "    \n",
    "    for e_name, properties in kg_data.items():\n",
    "        # Rule 1: Use 'label' if available\n",
    "        if 'label' in properties and properties['label']:\n",
    "            label_map[e_name] = properties['label'][0]\n",
    "            continue\n",
    "            \n",
    "        # Rule 2: Use 'sameAs' if available\n",
    "        if 'sameAs' in properties:\n",
    "            for uri in properties['sameAs']:\n",
    "                if isinstance(uri, str):\n",
    "                    label_map[e_name] = uri\n",
    "                    break  # Use the first valid sameAs\n",
    "            if e_name in label_map:\n",
    "                continue\n",
    "                \n",
    "        # Rule 3: For Person type, combine firstName and surname\n",
    "        if 'type' in properties and 'Person' in properties['type']:\n",
    "            first_name = properties.get('firstName', [''])[0]\n",
    "            surname = properties.get('surname', [''])[0]\n",
    "            if first_name and surname:\n",
    "                label_map[e_name] = f\"{first_name} {surname}\"\n",
    "                continue\n",
    "            elif first_name:\n",
    "                label_map[e_name] = first_name\n",
    "                continue\n",
    "            elif surname:\n",
    "                label_map[e_name] = surname\n",
    "                continue\n",
    "                \n",
    "        # Rule 4: Use 'title' if available\n",
    "        if 'title' in properties and properties['title']:\n",
    "            label_map[e_name] = properties['title'][0]\n",
    "            continue\n",
    "            \n",
    "        # Rule 5: Keep empty if none of the above\n",
    "        label_map[e_name] = None\n",
    "    \n",
    "    return label_map\n",
    "\n",
    "def replace_references(data, label_map):\n",
    "    \"\"\"Replace all entity ID references with their display labels\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: replace_references(v, label_map) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_references(item, label_map) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        if data in label_map:\n",
    "            return label_map[data] or data  # Use original if label is None\n",
    "    return data\n",
    "\n",
    "def process_kg_data(input_file, output_file):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        kg_data = json.load(f)\n",
    "    \n",
    "    label_map = create_label_mapping(kg_data)\n",
    "    transformed_data = replace_references(kg_data, label_map)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(transformed_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"../data/MSE-KG/KGPreprocess/MSE_KG_Wikidata.json\"\n",
    "    output_json = \"../data/MSE-KG/KGPreprocess/MSE_KG_Tail_Entity.json\"\n",
    "    process_kg_data(input_json, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_json(\"../data/MSE-KG/KGPreprocess/MSE_KG_Tail_Entity.json\", encoding=\"utf-8\").T\n",
    "df_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1d04f",
   "metadata": {},
   "source": [
    "## Process all relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the original JSON file\n",
    "with open(\"../data/MSE-KG/KGPreprocess/MSE_KG_Tail_Entity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 1: Build a mapping from property keys to their shortest label\n",
    "key_label_map = {}\n",
    "for key, value in data.items():\n",
    "    if (\n",
    "        isinstance(value, dict) \n",
    "        and \"label\" in value \n",
    "        and isinstance(value[\"label\"], list) \n",
    "        and value[\"label\"]\n",
    "    ):\n",
    "        # Select the shortest label (by character length, stripped of spaces)\n",
    "        shortest_label = min(value[\"label\"], key=lambda x: len(x.strip()))\n",
    "        key_label_map[key] = shortest_label.strip()\n",
    "\n",
    "# Step 2: Process all entity entries and replace keys using the label map\n",
    "transformed_data = {}\n",
    "\n",
    "for entity_key, entity_value in data.items():\n",
    "    # Only transform entities (e.g. \"E426504\"), not property definitions\n",
    "    if not entity_key.startswith(\"E\"):\n",
    "        transformed_data[entity_key] = entity_value\n",
    "        continue\n",
    "\n",
    "    new_entity = {}\n",
    "    for prop_key, prop_value in entity_value.items():\n",
    "        # Replace property name with its shortest label if available\n",
    "        new_key = key_label_map.get(prop_key, prop_key)\n",
    "        new_entity[new_key] = prop_value\n",
    "\n",
    "    transformed_data[entity_key] = new_entity\n",
    "\n",
    "# Step 3: Write the transformed output to a new JSON file\n",
    "with open(\"../data/MSE-KG/KGPreprocess/MSE_KG_Relation.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Transformation complete. Output saved to MSE_KG_Relation.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a19795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relation = pd.read_json(\"../data/MSE-KG/KGPreprocess/MSE_KG_Relation.json\", encoding=\"utf-8\").T\n",
    "df_relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3b300",
   "metadata": {},
   "source": [
    "## Process all head entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting E-names from the labeled JSON file\n",
    "file_path = \"../data/MSE-KG/KGPreprocess/MSE_KG_Relation.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f) \n",
    "\n",
    "# Create a dictionary to store entities starting with 'E'\n",
    "e_data = {}\n",
    "for key, value in data.items():  \n",
    "    if key.startswith('E'):\n",
    "        e_data[key] = value\n",
    "\n",
    "# Save the extracted entities to a new JSON file\n",
    "output_path = \"../data/MSE-KG/KGPreprocess/MSE_KG_Entity.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(e_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Extraction completed! Saved to {output_path}\")\n",
    "print(f\"Number of keys extracted: {len(e_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity = pd.read_json(\"../data/MSE-KG/KGPreprocess/MSE_KG_Entity.json\", encoding=\"utf-8\").T\n",
    "df_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c769fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_display_name(properties):\n",
    "    \"\"\"Get entity display name according to the five-step rule\"\"\"\n",
    "    # Rule 1: Use 'label' if available\n",
    "    if 'label' in properties and properties['label']:\n",
    "        return properties['label'][0]\n",
    "        \n",
    "    # Rule 2: Use 'sameAs' if available\n",
    "    if 'sameAs' in properties:\n",
    "        for uri in properties['sameAs']:\n",
    "            if isinstance(uri, str):\n",
    "                return uri  # Use the first valid sameAs\n",
    "    \n",
    "    # Rule 3: For Person type, combine firstName and surname\n",
    "    if 'type' in properties and 'Person' in properties['type']:\n",
    "        first_name = properties.get('firstName', [''])[0]\n",
    "        surname = properties.get('surname', [''])[0]\n",
    "        if first_name and surname:\n",
    "            return f\"{first_name} {surname}\"\n",
    "        elif first_name:\n",
    "            return first_name\n",
    "        elif surname:\n",
    "            return surname\n",
    "    \n",
    "    # Rule 4: Use 'title' if available\n",
    "    if 'title' in properties and properties['title']:\n",
    "        return properties['title'][0]\n",
    "    \n",
    "    # Rule 5: Return None if none of the above\n",
    "    return None\n",
    "\n",
    "def transform_kg_data(kg_data):\n",
    "    \"\"\"Transform knowledge graph data: replace outer keys and remove label/sameAs\"\"\"\n",
    "    new_kg_data = {}\n",
    "    \n",
    "    for e_name, properties in kg_data.items():\n",
    "        # Get display name\n",
    "        display_name = get_display_name(properties)\n",
    "        \n",
    "        # If no display name found, keep the original E-name\n",
    "        new_key = display_name if display_name is not None else e_name\n",
    "        \n",
    "        # Create new properties dictionary, excluding label and sameAs\n",
    "        new_properties = {\n",
    "            k: v for k, v in properties.items() \n",
    "            if k not in ['label', 'sameAs']\n",
    "        }\n",
    "        \n",
    "        # Add to new dictionary\n",
    "        new_kg_data[new_key] = new_properties\n",
    "    \n",
    "    return new_kg_data\n",
    "\n",
    "def process_kg_data(input_file, output_file):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        kg_data = json.load(f)\n",
    "    \n",
    "    # Transform data\n",
    "    transformed_data = transform_kg_data(kg_data)\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(transformed_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processing completed, results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"../data/MSE-KG/KGPreprocess/MSE_KG_Entity.json\"\n",
    "    output_json = \"../data/MSE-KG/KGPreprocess/MSE_KG_NL.json\"\n",
    "    process_kg_data(input_json, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88726804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_processed = pd.read_json(\"../data/MSE-KG/KGPreprocess/MSE_KG_NL.json\", encoding=\"utf-8\").T\n",
    "df_entity_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a866f",
   "metadata": {},
   "source": [
    "Triple Textualization: To support vector-based semantic retrieval, RDF triples can be converted into readable text fragments. For example, each triple can be transformed into a natural language sentence such as: \"<subject>'s <predicate> is <object>\". If the entities and relations in the knowledge graph have labels or descriptive properties, these can be leveraged to generate richer text. For instance, for a given entity, all its related triples can be compiled into a descriptive \"document\" (with the entity as the paragraph topic, including all its predicate-object pairs). In this way, the relevant facts about each entity or relation become indexable textual passages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7924f4a",
   "metadata": {},
   "source": [
    "## Triplets & Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92faae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def process_object_value(obj):\n",
    "    \"\"\"Clean object value: remove line breaks and extra whitespace.\"\"\"\n",
    "    obj = str(obj)\n",
    "    obj = re.sub(r'[\\r\\n]+', ' ', obj)         # Replace line breaks with space\n",
    "    obj = ' '.join(obj.strip().split())        # Remove extra spaces\n",
    "    return obj\n",
    "\n",
    "\n",
    "def convert_json_to_triples(json_path, \n",
    "                          output_rdf=\"../data/MSE-KG/KGPreprocess/triples_original.txt\",\n",
    "                          output_grouped_rdf=\"../data/MSE-KG/KGPreprocess/triples_paragraph.txt\"):\n",
    "    \"\"\"Convert labeled JSON to RDF triples\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "\n",
    "    # Clean subject URIs before processing\n",
    "    cleaned_data = {}\n",
    "    for subject_uri, props in data.items():\n",
    "        clean_uri = re.sub(r'[\\r\\n]+', ' ', subject_uri).strip()\n",
    "        cleaned_data[clean_uri] = props\n",
    "    data = cleaned_data\n",
    "\n",
    "    # Output structures\n",
    "    rdf_triples = []                                # Standard RDF triples\n",
    "    grouped_rdf = defaultdict(list)                 # Grouped RDF data\n",
    "\n",
    "    for subject_uri, props in data.items():\n",
    "        subj_label = subject_uri.split(\"/\")[-1]\n",
    "\n",
    "        for predicate, objects in props.items():\n",
    "            pred_label = predicate.split(\":\")[-1].replace(\"_\", \" \")\n",
    "\n",
    "            object_list = [objects] if not isinstance(objects, list) else objects\n",
    "            for obj in object_list:\n",
    "                # RDF triple (treat all object values as strings)\n",
    "                cleaned_obj = process_object_value(obj)\n",
    "                escaped = cleaned_obj.replace('<', '').replace('>', '')  # Remove any accidental angle brackets\n",
    "\n",
    "                rdf_triple = f\"<{subject_uri}>, <{predicate}>, <{escaped}>\"\n",
    "                rdf_triples.append(f\"({rdf_triple})\")  # 格式: (<s>, <p>, <o>)\n",
    "\n",
    "                # Grouped RDF triple (without trailing dot)\n",
    "                grouped_rdf[subject_uri].append(f\"({rdf_triple})\")\n",
    "\n",
    "    # Generate grouped RDF lines in the correct format\n",
    "    grouped_lines = []\n",
    "    for uri, triples in grouped_rdf.items():\n",
    "        # Join all triples for this subject with commas and wrap in outer parentheses\n",
    "        joined_triples = \", \".join(triples)\n",
    "        grouped_lines.append(f\"({joined_triples})\")\n",
    "\n",
    "    # Write output files\n",
    "    def write_file(path, data):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(data))\n",
    "\n",
    "    write_file(output_rdf, rdf_triples)\n",
    "    write_file(output_grouped_rdf, grouped_lines)\n",
    "\n",
    "    print(f\"Conversion complete:\\n\"\n",
    "          f\"- RDF triples: {len(rdf_triples):,}\\n\"\n",
    "          f\"- Grouped RDF triples: {len(grouped_lines):,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_json_to_triples(\"../data/MSE-KG/KGPreprocess/MSE_KG_NL.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8ebfc",
   "metadata": {},
   "source": [
    "# Verbalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecfcec",
   "metadata": {},
   "source": [
    "## Sentence-Level Verbalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalization_prompt=\"\"\"\n",
    "## Role\n",
    "You are a knowledge graph verbalization expert that converts triples into natural English sentences while strictly following these rules:\n",
    "\n",
    "## Instructions:\n",
    "1. **Preservation Rule**  \n",
    "   - Subject and object must appear exactly as in the original triple\n",
    "2. **Predicate Handling**  \n",
    "   - Maintain the original meaning.  \n",
    "   - Prefer using words from the original predicate.  \n",
    "   - Make the connection sound natural in English\n",
    "3. Ensure that the generated sentence is grammatically correct in English.\n",
    "4. Directly output only the converted sentence without any additional text or explanation.\n",
    "\n",
    "\n",
    "## Examples\n",
    "### Standard Case\n",
    "**Triple:**  \n",
    "`(<PyTorch>, <developedBy>, <Meta AI Research>)`\n",
    "\n",
    "**Output:**  \n",
    "PyTorch is developed by Meta AI Research.\n",
    "\n",
    "### Complex Case\n",
    "**Triple:**  \n",
    "`(<Lewis, Patrick, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" Advances in neural information processing systems 33 (2020): 9459-9474.>, <introduced>, <Retrieval-Augmented Generation (RAG) model>)`\n",
    "\n",
    "**Output:**  \n",
    "\"Lewis, Patrick, et al. 'Retrieval-augmented generation for knowledge-intensive nlp tasks.' Advances in neural information processing systems 33 (2020): 9459-9474.\" introduced Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "**Triple:**  \n",
    "`(<subject>, <predicate>, <object>)`\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSeek client\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,  # Replace with your actual API key\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# File paths\n",
    "input_file = \"../data/MSE-KG/KGPreprocess/triples_original.txt\"\n",
    "output_file = \"../data/MSE-KG/KGPreprocess/verbalized_triples.txt\"\n",
    "\n",
    "# Batch settings\n",
    "BATCH_SIZE = 100  # Adjust based on token size\n",
    "\n",
    "# Load all input lines\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "total_lines = len(lines)\n",
    "\n",
    "# Count already processed lines\n",
    "start_line = 0\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        start_line = sum(1 for _ in f)\n",
    "\n",
    "\n",
    "# Start batch processing\n",
    "with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "    for i in range(start_line, total_lines, BATCH_SIZE):\n",
    "        batch = lines[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # Construct prompt\n",
    "        prompt = verbalization_prompt + \"\\n\\n\"\n",
    "        for idx, triple in enumerate(batch, start=1):\n",
    "            prompt += f\"{idx}. {triple}\\n\"\n",
    "        prompt += \"\\nReturn each sentence on a new line.\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                stream=False\n",
    "            )\n",
    "            result_text = response.choices[0].message.content.strip()\n",
    "            results = result_text.split(\"\\n\")\n",
    "            \n",
    "            # Write output lines\n",
    "            print(\"\\nGenerated verbalizations:\")\n",
    "            for res_line in results:\n",
    "                print(f\"- {res_line.strip()}\")\n",
    "                outfile.write(res_line.strip() + \"\\n\")\n",
    "            \n",
    "            # Handle potential missing responses\n",
    "            if len(results) < len(batch):\n",
    "                for _ in range(len(batch) - len(results)):\n",
    "                    outfile.write(\"ERROR: Missing output\\n\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Write errors for entire batch\n",
    "            for _ in batch:\n",
    "                outfile.write(\"ERROR: Processing failed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41cc27",
   "metadata": {},
   "source": [
    "##  Paragraph-Level Verbalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e86cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_prompt = \"\"\"\n",
    "You are an assistant that generates descriptive paragraph chunks about a given entity based on a list of RDF-style triples. Each triple shares the same head entity, and your goal is to verbalize these triples into a coherent, natural English paragraph by following these instructions:\n",
    "\n",
    "1. Begin the paragraph directly with the head entity name\n",
    "2. Use head/tail entities exactly as given\n",
    "3. Generate appropriate connecting language from predicates\n",
    "4. Avoid unnecessary repetition of the head entity\n",
    "5. Create a single, fluent paragraph mentioning every triple\n",
    "6. Never comment on or question triple validity\n",
    "7. Include all triples, even if inconsistent\n",
    "8. Exclude all notes or explanations\n",
    "\n",
    "Example:\n",
    "\n",
    "Input triples:\n",
    "((<Alan Turing>, <instance of>, <human>), (<Alan Turing>, <given name>, <Alan>), (<Alan Turing>, <family name>, <Turing>), (<Alan Turing>, <occupation>, <mathematician>), (<Alan Turing>, <occupation>, <computer scientist>), (<Alan Turing>, <occupation>, <cryptanalyst>), (<Alan Turing>, <educated at>, <University of Cambridge>), (<Alan Turing>, <educated at>, <Princeton University>), (<Alan Turing>, <date of birth>, <1912-06-23>), (<Alan Turing>, <date of death>, <1954-06-07>), (<Alan Turing>, <notable work>, <Turing machine>), (<Alan Turing>, <notable work>, <On Computable Numbers>))\n",
    "\n",
    "Output paragraph:\n",
    "Alan Turing is a human whose given name is Alan and family name is Turing. He was born on June 23, 1912, and died on June 7, 1954. Turing was a mathematician, computer scientist, and cryptanalyst. He studied at both the University of Cambridge and Princeton University. His most notable works include the concept of the Turing machine and the influential paper On Computable Numbers.\n",
    "\n",
    "Task:\n",
    "Verbalize the following new triples into a coherent paragraph. Output only the final paragraph without any additional text or explanation.\n",
    "\n",
    "Input triples: ((subject A, predicate A, object A), (subject A, predicate B, object B), (subject A, predicate C, object C)) ...\n",
    "Output paragraph:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==== Setup ====\n",
    "BATCH_SIZE = 100  # Start with a small batch size for testing\n",
    "INPUT_FILE = \"../data/MSE-KG/KGPreprocess/triples_paragraph.txt\"\n",
    "OUTPUT_FILE = \"../data/MSE-KG/KGPreprocess/verbalized_paragraphs.txt\"  \n",
    "LOG_FILE = \"../log/verbalization_test.log\"\n",
    "\n",
    "\n",
    "# ==== Logging ====\n",
    "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='w',  # Overwrite old logs during testing\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# ==== DeepSeek Client ====\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,  # Replace with your actual API key\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# ==== Load Input File ====\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE} not found!\")\n",
    "\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    paragraphs = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "total_lines = len(paragraphs)\n",
    "logging.info(f\"Total lines to process: {total_lines}\")\n",
    "print(f\"\\n🚀 Starting processing. Total lines to process: {total_lines}\\n\")\n",
    "\n",
    "# ==== Processing ====\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out: \n",
    "    for i in range(0, total_lines, BATCH_SIZE):\n",
    "        batch_start_time = time.time()\n",
    "        batch = paragraphs[i:i + BATCH_SIZE]\n",
    "        line_range = f\"{i + 1}-{i + len(batch)}\"\n",
    "        \n",
    "        # Print progress information\n",
    "        progress = (i / total_lines) * 100\n",
    "        elapsed_time = time.time() - start_time\n",
    "        estimated_total = (elapsed_time / (i + 1)) * total_lines if i > 0 else 0\n",
    "        remaining_time = estimated_total - elapsed_time\n",
    "        \n",
    "        print(f\"\\n📊 Progress: {progress:.1f}% | Processed: {i}/{total_lines} | \"\n",
    "              f\"Elapsed: {elapsed_time:.1f}s | Remaining: {remaining_time:.1f}s\")\n",
    "        print(f\"🔄 Processing batch {line_range}...\")\n",
    "        \n",
    "        logging.info(f\"Processing batch {line_range}\")\n",
    "        batch_output = []\n",
    "        \n",
    "        try:\n",
    "            for j, triple_group in enumerate(batch, 1):\n",
    "                print(f\"\\n🔍 Input triples ({j}/{len(batch)}):\")\n",
    "                print(triple_group[:200] + (\"...\" if len(triple_group) > 200 else \"\"))\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"deepseek-chat\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates fluent paragraphs from structured data.\"},\n",
    "                        {\"role\": \"user\", \"content\": chunk_prompt + triple_group}\n",
    "                    ],\n",
    "                    temperature=0.3  \n",
    "                )\n",
    "                result = response.choices[0].message.content.strip()\n",
    "                \n",
    "                print(\"\\n💬 Generated result:\")\n",
    "                print(result)\n",
    "                \n",
    "                batch_output.append(result)\n",
    "                processed_count += 1\n",
    "            \n",
    "            # Write output\n",
    "            for output in batch_output:\n",
    "                f_out.write(output + \"\\n\")  # Add blank lines between paragraphs\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            logging.info(f\"Successfully processed batch {line_range} in {batch_time:.2f}s\")\n",
    "            print(f\"✅ Batch {line_range} completed (Time: {batch_time:.1f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in batch {line_range}: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            print(f\"❌ Error: {error_msg}\")\n",
    "            for _ in batch:\n",
    "                f_out.write(\"ERROR: Could not process this triple group\\n\")\n",
    "\n",
    "# Final statistics\n",
    "total_time = time.time() - start_time\n",
    "avg_time = total_time / processed_count if processed_count > 0 else 0\n",
    "\n",
    "print(f\"\\n🎉 Processing completed! Total time: {total_time:.1f} seconds\")\n",
    "print(f\"📝 Successfully processed: {processed_count}/{total_lines} items\")\n",
    "print(f\"⏱️ Average processing time per item: {avg_time:.2f} seconds\")\n",
    "\n",
    "logging.info(f\"Test processing completed in {total_time:.2f}s\")\n",
    "logging.info(f\"Successfully processed {processed_count}/{total_lines} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa2825",
   "metadata": {},
   "source": [
    "# Post Processing for Sentence-Level Verbalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ed0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Cleans a text file by:\n",
    "    1. Deleting lines starting with \"let\" or \"here\" (case insensitive)\n",
    "    2. Removing all blank lines\n",
    "    3. Removing leading numbers followed by dots (e.g., \"100. \")\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            # Skip lines starting with let/here (case insensitive)\n",
    "            lower_line = line.lower().strip()\n",
    "            if lower_line.startswith(('let ', 'here ')):\n",
    "                continue\n",
    "            \n",
    "            # Skip blank lines\n",
    "            stripped_line = line.strip()\n",
    "            if not stripped_line:\n",
    "                continue\n",
    "            \n",
    "            # Remove leading number and dot (e.g., \"1. \", \"100. \")\n",
    "            cleaned_line = re.sub(r'^\\d+\\.\\s*', '', stripped_line)\n",
    "            \n",
    "            # Write the processed line to output\n",
    "            outfile.write(cleaned_line + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = '../data/MSE-KG/verbalized_triples.txt'\n",
    "    output_filename = '../data/MSE-KG/verbalized_triples_np.txt'\n",
    "    \n",
    "    clean_file(input_filename, output_filename)\n",
    "    print(f\"Processing complete. Results saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
